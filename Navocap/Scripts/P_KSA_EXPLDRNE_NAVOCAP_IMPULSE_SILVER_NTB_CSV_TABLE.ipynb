{"cells":[{"cell_type":"markdown","source":["# Copie de tous les csv du lkh vers les tables sans doublons"],"metadata":{},"id":"9bb3f5b4-2c9d-4157-86da-8f4108c08fff"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from delta.tables import DeltaTable\n","import datetime\n","from py4j.java_gateway import java_import\n","\n","# Initialiser Spark\n","spark = SparkSession.builder \\\n","    .appName(\"Lakehouse Data Deduplication\") \\\n","    .getOrCreate()\n","\n","# Importer les classes FileSystem et Path de Hadoop\n","java_import(spark._jvm, \"org.apache.hadoop.fs.*\")\n","\n","# Définir le chemin du dossier\n","folder_path = \"abfss://84c84e57-9f4b-4896-8968-fa2878d883be@onelake.dfs.fabric.microsoft.com/7842e0c1-a56f-4c42-bd64-ce8813413131/Files\"\n","hdfs_path = spark._jvm.Path(folder_path)\n","fs = hdfs_path.getFileSystem(spark._jsc.hadoopConfiguration())\n","\n","# CHANGER LA VALEUR 7 POUR LE PREMIER RUN POUR AJOUTER TOUS LES FICHIERS DANS LA TABLE\n","# Définir la date de référence pour les fichiers de moins de 7 jours\n","reference_date = datetime.datetime.now() - datetime.timedelta(days=24)\n","\n","# Lister les fichiers récents\n","file_statuses = fs.listStatus(hdfs_path)\n","recent_files = [status.getPath().toString() for status in file_statuses if datetime.datetime.fromtimestamp(status.getModificationTime() / 1000) > reference_date]\n","\n","# Lire les fichiers récents\n","if recent_files:\n","    df = spark.read.csv(recent_files, header=True, inferSchema=True, sep=';')\n","    # Supprimer les doublons\n","    df_no_duplicates = df.dropDuplicates()\n","\n","    # Spécifier le chemin de la table Delta dans le Lakehouse\n","    delta_table_path = \"abfss://84c84e57-9f4b-4896-8968-fa2878d883be@onelake.dfs.fabric.microsoft.com/7842e0c1-a56f-4c42-bd64-ce8813413131/Tables/navocap\"\n","\n","    # Vérifier si la table Delta existe déjà\n","    if DeltaTable.isDeltaTable(spark, delta_table_path):\n","        # Charger la table existante\n","        delta_table = DeltaTable.forPath(spark, delta_table_path)\n","        \n","        # Faire un merge pour enlever les doublons entre la source et la table cible\n","        delta_table.alias(\"target\").merge(\n","            df_no_duplicates.alias(\"source\"),\n","            \"target.Date = source.Date AND target.MatriculeACC = source.MatriculeACC AND target.HoraireReelEntreeZoneArret = source.HoraireReelEntreeZoneArret AND target.HoraireDepartApplicable = source.HoraireDepartApplicable AND target.TempsEchangePassager = source.TempsEchangePassager\"\n","        ).whenNotMatchedInsertAll().execute()\n","        print(\"merge\")\n","    else:\n","        # Si la table n'existe pas, créez-la\n","        df_no_duplicates.write.format(\"delta\").save(delta_table_path)\n","        print(\"new table\")\n","\n","    print(\"That's all folks!\")\n","\n","# Arrêter la session Spark\n","spark.stop()\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3284966-6e33-44fd-84a1-5fc643305b05"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"7842e0c1-a56f-4c42-bd64-ce8813413131","default_lakehouse_name":"P_KSA_DRNE_NAVOCAP_BRONZESILVER_LKH","default_lakehouse_workspace_id":"84c84e57-9f4b-4896-8968-fa2878d883be","known_lakehouses":[{"id":"7842e0c1-a56f-4c42-bd64-ce8813413131"}]}}},"nbformat":4,"nbformat_minor":5}